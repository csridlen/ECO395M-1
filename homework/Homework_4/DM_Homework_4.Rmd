---
title: "DM_Homework_4"
author: "Christina Ridlen"
date: "`r Sys.Date()"
output: md_document
---

```{r libraries, include = FALSE}
library(stats)
library(tidyverse)
library(factoextra)
library(ggpubr)
library(arules)
library(arulesViz)
library(htmlwidgets)
setwd("C:/Users/tinar/ECO395M-Homework/homework")
```

# Problem 1: Clustering and PCA

## Overview

The properties of 6500 different bottles of wine are included in the `wine` dataset. Along with 11 chemical properties, we have an indicator for whether the wine is red or white, and the quality of the wine, rated on a scale from 1-10. The goal here is to use unsupervised learning methods to categorize the information in the dataset. Additionally, we want to determine if the analysis can distinguish higher quality wines from lower quality ones.

## Clustering Analysis

First, I attempt to solve this problem through cluster analysis. Specifically, I will be using the K-means algorithm. I normalize the data with the `scale` function from the base R library. This function centers the numeric columns of the dataset. To see if this algorithm distinguishes reds from whites, I set the number of clusters to 2.

```{r wine_data, include = FALSE}
wine <- read.csv("../data/wine.csv", header = TRUE)
wine["id"] = c(1:6497)
X <- scale(subset(wine, select = -c(color, quality, id)))
```

```{r kmeans_color, include = FALSE}
# Scale variables

km.color <- kmeans(X, centers = 2, nstart = 20, iter.max = 50)

# Join wine with cluster
col_cluster <- data.frame(km.color$cluster)
col_cluster["id"] = c(1:6497)
wine_2 <- merge(wine, col_cluster)

### Summarize the two clusters
# Summarize one cluster
wine_2 %>%
  filter(km.color.cluster == 1) %>%
  group_by(color) %>%
  summarize(n = n())

# Summarize other cluster
wine_2 %>%
  filter(km.color.cluster == 2) %>%
  group_by(color) %>%
  summarize(n = n())
par(mfrow = c(1,2), bg = 'gray')
```

```{r plot_colorkm, echo = FALSE}

plot(X, 
     col = km.color$cluster,
     main = "K-Means with two clusters", 
     xlab = " ", 
     ylab = " ", pch = 20)

plot(X,
     col = wine$color,
     main = "Wine data by color of wine", 
     xlab = " ", 
     ylab = " ",
     pch = 20)
```

Here we see that the algorithm almost perfectly distinguishes reds from whites. It only misses some whites that are similar to reds, so those white wines must have more sugar or more acidity.

### Choosing the right number of clusters

```{r kmeans_wine, include = FALSE}

# Find number of clusters
wss <- 0
set.seed(123)
for (i in 1:15) {
  km.wine <- kmeans(X, centers = i, nstart = 20, iter.max = 50)
  wss[i] <- km.wine$tot.withinss
}
```

```{r elbow_plot, echo = FALSE}
par(mfrow = c(1,1), bg = "white")
plot(1:15, wss, type = "b")
```

The crook of the elbow seems to occur at 4, or the marginal value of the next k seems to peak at k = 4. This is something we can work with; consider there to be 4 types of wine: high quality red wine, high quality white wine, low quality red wine, and low quality white wine. I create a variable `color_quality`, which is an indicator of whether the wine is red or white and of high quality ($\geq$ 5) or low quality (\< 5), to distinguish these four types. The results are summarized in the two plots below. See the table for reference as to what the colors mean in the left plot.

```{r kmeans_quality, include = FALSE}
k <- 4


# Now run kmeans with centers = 4
km.quality <- kmeans(X, centers = 4, nstart = 20, iter.max = 50)
km.quality
plot(X, col = km.quality$cluster, main = "K-means with 4 clusters", xlab = " ", ylab = " ", pch = 20)

# Let's assume 4 clusters are high and low quality red and white.

# Create indicator for high quality white/red, low quality white/red

col_quality <- data.frame(km.quality$cluster)
col_quality["id"] = c(1:6497)
wine_4 <- merge(wine, col_quality)

wine_4 <- wine_4 %>%
  mutate(color_quality = ifelse(color == "red" & quality >= 5, 1, ifelse(color == "red" & quality < 5, 2, ifelse(color == "white" & quality >= 5, 3, 4))))
```

```{r kmeans_quality_plots, echo = FALSE}

par(mfrow = c(1,2))
plot(X,
     col = wine_4$color_quality,
     main = "Four qualities of wine",
     xlab = " ",
     ylab = " ",
pch = 20)

plot(X,
     col = km.quality$cluster,
     main = "K means with 4 clusters",
     xlab = " ",
     ylab = " ",
     pch = 20)
```

| Category Name           | Indicator | Corresponding Color |
|-------------------------|-----------|---------------------|
| High quality red wine   | 1         | Black               |
| Low quality red wine    | 2         | Red                 |
| High quality white wine | 3         | Green               |
| Low quality white wine  | 4         | Blue                |

It appears that K means is good at distinguishing high quality wine wines and goes half and half with the quality of red wine. It's possible that wine snobs consider all red wine to be high quality when a lot of it has characteristics of a low quality wine. Or, the wines that K-means considers to be high or low quality (based on chemical properties) is different than what wine snobs would consider to be high or low quality.

## Principal Components Analysis

```{r pr_wine, include = FALSE}
pr.wine <- prcomp(X, scale = TRUE)



wine_rotation <- pr.wine$rotation %>%
  as.data.frame() %>%
  rownames_to_column('Chemical Component')
```

```{r sumtab, echo = FALSE}
sum.pca <- summary(pr.wine)
pca_importance <- function(x) {
  vars <- x$sdev^2
  vars <- vars/sum(vars)
  rbind(`Standard deviation` = x$sdev, `Proportion of Variance` = vars, 
      `Cumulative Proportion` = cumsum(vars))
}

pca_imp <- pca_importance(sum.pca)
colnames(pca_imp) <- c("PC1", "PC2", "PC3", "PC4", "PC5", "PC6", "PC7", "PC8", "PC9", "PC10", "PC11")
knitr::kable(pca_imp)

```

By summarizing the statistics of the principal components, we learn that:

-   First 2 principal components explain about 50% of the cumulative variance in the dataset

-   First 4 principal components explain about 80% of the cumulative variance in the dataset

```{r pr_comp_wine, include = FALSE}
pr.wine$rotation <- -1*pr.wine$rotation
pr.wine$rotation


wine <- wine %>%
  mutate(color_quality = ifelse(color == "red" & quality >= 5, 1, ifelse(color == "red" & quality < 5, 2, ifelse(color == "white" & quality >= 5, 3, 4))))


```

```{r qplot_wine_pca, echo = FALSE}
wine <- merge(wine, pr.wine$x[, 1:11], by = "row.names")
wine = subset(wine, select = -c(Row.names))

qplot(pr.wine$x[, 1], 
      pr.wine$x[, 2], 
      color = wine$color, 
      xlab = "PC 1", 
      ylab = "PC 2")
```

```{r qplot_quality, include = FALSE}
qplot(pr.wine$x[, 1],
      pr.wine$x[, 2],
      color = wine$quality,
      xlab = "PC 1",
      ylab = "PC 2")
```

We see that PCA performs similarly for categorizing wine color. Interpreting quality is a little more ambiguous. Additionally, PCA shows that four principal components are the most explanatory of the variance in the data. However, if we want to learn anything through visualization, we would have to plot $\binom{4}{2} = 6$ different plots to learn exactly how those four different components separate the data.

## Conclusion

**Clustering works better here for distinguishing reds with whites.** The process is much more simple and pretty efficient at characterizing the wines. PCA is not as useful here because there is a lot of overlap between the points. If the data could be organized into different components more evenly, the results would be better for interpretation.

However, PCA suggests that the four clusters found in K-means are most explanatory of the data. This might confirm the "four-types" hypothesis I proposed from K-means.

# Problem 2: Market Segmentation

## Overview

In this problem, I want to use exploratory data analysis to optimize marketing schemes for NutrientH2O's twitter followers. There are about 7900 Twitter users that follow the account and their tweets were observed for seven days. For each tweet observed, the general topic of the tweet was placed into one of 36 categories, such as "current events", "travel", "photo sharing." I use K-means clustering to find interesting results from tweet category counts.

## Organizing Followers by Tweets

```{r market_segmentation_data, include = FALSE}
social_marketing <- read.csv("../data/social_marketing.csv")
glimpse(social_marketing)
X <- subset(social_marketing, select = -X)

# Normalize phrase counts to phrase frequencies
X <- X/rowSums(X)
```

In terms of what we will do with the data, we have to find a suitable unsupervised method. Clustering makes the most sense here because we don't know much about the relationship between the categories. Understanding the components would be really tough. Similarly, in customer data a hierarchical model might not work as well for interpretability; there are too many observations. So, I proceed with K-means.

First, I normalize tweet counts as tweet *frequencies* to scale the observations for cluster analysis. To find the optimal number of clusters $k$ I produce a scree plot, or a plot showing the improvement in the within-sum-of-squares of the clusters as the number of clusters increases. I use the "elbow test," or I try to find the number of $k$ in the plot where there is a "crook" in the elbow.

```{r kmeans_market, include = FALSE}
# Focus on top 10 categories
# X <- subset(X[, names(top_10)], select = -chatter)

wss <- 0
set.seed(123)
for (i in 1:11) {
  km.social <- kmeans(X, centers = i, nstart = 20, iter.max = 50)
  wss[i] <- km.social$tot.withinss
}
```

```{r elbow_2, echo = FALSE}
plot(1:11, wss, type = 'b')
```

The scree plot indicates we should use two clusters.

### K-means with 2 clusters

Since there are so many observations and overlap within accounts, plotting the data was not very useful.

```{r social_km2, include = FALSE}

km.social <- kmeans(X, centers = 2, nstart = 20, iter.max = 50)


# Merge with clusters
X_clust <- rename(merge(X, km.social$cluster, by = "row.names"), cluster = y)


X_tbl <- X_clust %>%
  group_by(cluster) %>%
  select(-Row.names) %>%
  summarize_all(mean) %>%
  select(-c(cluster, chatter, uncategorized))
  
X_tbl <- as.data.frame(t(as.data.frame(X_tbl)))

X_tbl <- tibble::rownames_to_column(X_tbl)

colnames(X_tbl) <- c("Category", "cluster_1", "cluster_2") 

X_tbl$Category = as.factor(X_tbl$Category)

X_tbl <- X_tbl %>%
mutate(top_categoryc1 = ifelse(cluster_1 %in% head(sort(X_tbl$cluster_1, decreasing = TRUE), 5), TRUE, FALSE),
       top_categoryc2 = ifelse(cluster_2 %in% head(sort(X_tbl$cluster_2, decreasing = TRUE), 5), TRUE, FALSE))


p1 <- ggplot(X_tbl) + 
  geom_col(aes(x = Category, y = cluster_1, fill = top_categoryc1), show.legend = FALSE) + coord_flip() + 
  ylab("Average Frequency of Tweets") + 
  ggtitle("Cluster 1") + 
  scale_fill_manual(values = c("#999999", "#FF9999"))

p2 <- ggplot(X_tbl) + 
  geom_col(aes(x = Category, y = cluster_2, fill = top_categoryc2), show.legend = FALSE) + coord_flip() + 
  ggtitle("Cluster 2") + 
  ylab("Average Frequency of Tweets") + 
  scale_fill_manual(values = c("#999999", "#FF9999"))
  
figure <- ggarrange(p1, 
                    p2,
                    ncol = 2,
                    nrow = 1)  
figure  
```

## Conclusion

It appears that we can categorize NutrientH20's followers into two categories by their tweets' categorical frequencies. In cluster 1, the top 5 tweet categories are photo sharing, personal fitness, outdoors, health/nutrition, and cooking. These seem like the types of people that the account can market to by promoting the health and fitness benefits of the drink. Maybe they can post a picture of someone hiking with their NutrientH20 drink!

In cluster 2, the top categories are travel, sports, politics, photo sharing, and current events. These are also probably the top categories for young men on Twitter in general. To market to this audience, NutrientH20 should just try to stay relevant in Twitter culture. People in cluster 2 would totally buy NutrientH20 just because one of their funny tweets went viral.

# Problem 3: Market Basket Analysis

## Overview

In the `groceries` dataset, each row is a "basket" of grocery items purchased. The goal here is to find the most interesting association rules between items purchased. This is also known as market basket analysis.

```{r p3_data_pp, include = FALSE, cache = TRUE}

groceries <- read_delim(
  file = "../data/groceries.txt",
  delim = "\t",
  col_names = FALSE
)

for(i in 1:length(groceries$X1)){
  for(j in 1:length(groceries$X1[i])){
    groceries$X1[[i]] = strsplit(groceries$X1[[i]], ",")
    groceries$X1[[i]][[j]] = strsplit(groceries$X1[[i]][[j]], "''")
  }
}

groceries$basket = as.factor(c(1:9835))

for(i in 1:length(groceries$X1)){
  for(j in 1:length(groceries$X1[i]))
  groceries$X1[i][[j]] = unlist(groceries$X1[i])
}


grocery_list <- groceries$X1
names(grocery_list) <- as.factor(c(1:9835))

basket_trans <- as(grocery_list, "transactions")
summary(basket_trans)
```

## Working with the data

First, I summarize the top 5 most purchased items in each basket in a bar plot.

```{r item_freq, echo = FALSE}
itemFrequencyPlot(basket_trans, topN = 5, col = rainbow(10), type = "absolute")
```

To calculate the association rules, I used the `apriori` function in R, choosing a support of 0.0009 and a confidence of 0.5. So I want rules that happen about 0.09% of the time and that we are about 50% confident will occur in a given transaction. I choose these parameters because I want to isolate rules that are less common (low support) but have a high confidence of occurring together (confidence). This just makes our results more interesting.

```{r data_ap, include = FALSE}



# apriori

basketrules = apriori(basket_trans, 
                      parameter = list(support = 0.0009, confidence = 0.5,
                           maxlen = 10, target = "rules"))
inspect(basketrules)




```

In this table, I summarize the top 10 rules with the highest lifts.

```{r h_lift, echo = FALSE}
# Find rules with the highest lifts

brules_lift <- as.data.frame(inspect(head(sort(basketrules, by = "lift"), 10)))
brules_lift <- brules_lift[, -2]
rownames(brules_lift) <- NULL
knitr::kable(brules_lift)
```

## Results

If we want people to purchase more vegetables, which grocery items should they be placed by?

```{r arules_veg, include = FALSE}
# Inspect rules that have "other vegetables" on the rhs

rules_veg <- apriori(basket_trans,
                     parameter = list(supp = 0.001, conf = 0.5, maxlen = 10),
                     appearance = list(rhs = "other vegetables"))

rules_veg_tabl <- as.data.frame(inspect(head(sort(rules_veg, by = "lift"), 5)))
rules_veg_tabl <- rules_veg_tabl[, -2]
rownames(rules_veg_tabl) <- NULL

rules_veg_graph <- plot(rules_veg, method = "graph")

# What about sodas

rules_soda <- apriori(basket_trans,
                      parameter = list(supp = 0.001, conf = 0.5, minlen = 2),
                      appearance = list(rhs = "soda"))
inspect(head(sort(rules_soda, by = "lift")))
rules_soda_graph <- plot(rules_soda[1:10], method = "graph")

```

Below I have summarized in two graphs rules that include "other vegetables" and "soda" on the right hand side. If we want people to consume more vegetables, we can place these food items near the vegetables. Similarly, if a soda company wants more people to buy its product, it can place their sodas next to these products. It seems that people mostly consumer sodas with other drinks. For calculating the association rules for these two products, I let the support equal 0.001 and the confidence equal 0.5, using a similar logic as before.

```{r arules_graphs, echo = FALSE}
knitr::kable(rules_veg_tabl)
ggarrange(rules_veg_graph,
          rules_soda_graph,
          ncol = 2,
          nrow = 1)

```

Overall, the results are pretty standard because we're dealing with grocery items. However, the `apriori` algorithm on this dataset could be useful for suppliers of common grocery items.
