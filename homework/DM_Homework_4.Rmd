---
title: "DM_Homework_4"
author: "Christina Ridlen"
date: "4/25/2022"
output: html_document
---

```{r libraries, include = FALSE}
library(stats)
library(tidyverse)
library(factoextra)
library(ggpubr)
library(arules)
setwd("C:/Users/tinar/ECO395M-Homework/homework")
```

# Problem 1: Clustering and PCA

## Overview

The properties of 6500 different bottles of wine are included in the `wine` dataset. Along with 11 chemical properties, we have an indicator for whether the wine is red or white, and the quality of the wine, rated on a scale from 1-10. The goal here is to use unsupervised learning methods to categorize the information in the dataset. Additionally, we want to determine if the analysis can distinguish higher quality wines from lower quality ones.

## Clustering Analysis

First, I attempt to solve this problem through cluster analysis. Specifically, I will be using the K-means algorithm. I normalize the data with the `scale` function from the base R library. This function centers the numeric columns of the dataset. To see if this algorithm distinguishes reds from whites, I set the number of clusters to 2.

```{r wine_data, include = FALSE}
wine <- read.csv("../data/wine.csv", header = TRUE)
wine["id"] = c(1:6497)
X <- scale(subset(wine, select = -c(color, quality, id)))
```

```{r kmeans_color, include = FALSE}
# Scale variables

km.color <- kmeans(X, centers = 2, nstart = 20, iter.max = 50)

# Join wine with cluster
col_cluster <- data.frame(km.color$cluster)
col_cluster["id"] = c(1:6497)
wine_2 <- merge(wine, col_cluster)

### Summarize the two clusters
# Summarize one cluster
wine_2 %>%
  filter(km.color.cluster == 1) %>%
  group_by(color) %>%
  summarize(n = n())

# Summarize other cluster
wine_2 %>%
  filter(km.color.cluster == 2) %>%
  group_by(color) %>%
  summarize(n = n())

```

```{r plot_colorkm, echo = FALSE}
par(mfrow = c(1,2), bg = 'gray')
plot(X, 
     col = km.color$cluster,
     main = "K-Means with two clusters", 
     xlab = " ", 
     ylab = " ", pch = 20)

plot(X,
     col = wine$color,
     main = "Wine data by color of wine", 
     xlab = " ", 
     ylab = " ",
     pch = 20)
```

Here we see that the algorithm almost perfectly distinguishes reds from whites. It only misses some whites that are similar to reds, so those white wines must have more sugar or more acidity.

### Choosing the right number of clusters

```{r kmeans_wine, include = FALSE}

# Find number of clusters
wss <- 0
set.seed(123)
for (i in 1:15) {
  km.wine <- kmeans(X, centers = i, nstart = 20, iter.max = 50)
  wss[i] <- km.wine$tot.withinss
}
```

```{r elbow_plot, echo = FALSE}
plot(1:15, wss, type = "b")
```

The crook of the elbow seems to occur at 4, or the marginal value of the next k seems to peak at k = 4. This is something we can work with; consider there to be 4 types of wine: high quality red wine, high quality white wine, low quality red wine, and low quality white wine. I create a variable `color_quality`, which is an indicator of whether the wine is red or white and of high quality ($\geq$ 5) or low quality (\< 5), to distinguish these four types. The results are summarized in the two plots below. See the table for reference as to what the colors mean in the left plot.

```{r kmeans_quality, include = FALSE}
k <- 4


# Now run kmeans with centers = 4
km.quality <- kmeans(X, centers = 4, nstart = 20, iter.max = 50)
km.quality
plot(X, col = km.quality$cluster, main = "K-means with 4 clusters", xlab = " ", ylab = " ", pch = 20)

# Let's assume 4 clusters are high and low quality red and white.

# Create indicator for high quality white/red, low quality white/red

col_quality <- data.frame(km.quality$cluster)
col_quality["id"] = c(1:6497)
wine_4 <- merge(wine, col_quality)

wine_4 <- wine_4 %>%
  mutate(color_quality = ifelse(color == "red" & quality >= 5, 1, ifelse(color == "red" & quality < 5, 2, ifelse(color == "white" & quality >= 5, 3, 4))))
```

```{r kmeans_quality_plots, echo = FALSE}

par(mfrow = c(1,2))
plot(X,
     col = wine_4$color_quality,
     main = "Four qualities of wine",
     xlab = " ",
     ylab = " ",
pch = 20)

plot(X,
     col = km.quality$cluster,
     main = "K means with 4 clusters",
     xlab = " ",
     ylab = " ",
     pch = 20)
```

| Category Name           | Indicator | Corresponding Color |
|-------------------------|-----------|---------------------|
| High quality red wine   | 1         | Black               |
| Low quality red wine    | 2         | Red                 |
| High quality white wine | 3         | Green               |
| Low quality white wine  | 4         | Blue                |

It appears that K means is good at distinguishing high quality wine wines and goes half and half with the quality of red wine. It's possible that wine snobs consider all red wine to be high quality when a lot of it has characteristics of a low quality wine. Or, the wines that K-means considers to be high or low quality (based on chemical properties) is different than what wine snobs would consider to be high or low quality.

## Principal Components Analysis

```{r pr_wine, include = FALSE}
pr.wine <- prcomp(X, scale = TRUE)
summary(pr.wine)


wine_rotation <- pr.wine$rotation %>%
  as.data.frame() %>%
  rownames_to_column('Chemical Component')
```

```{r sumtab, echo = FALSE}
sum.pca <- summary(pr.wine)
pca_importance <- function(x) {
  vars <- x$sdev^2
  vars <- vars/sum(vars)
  rbind(`Standard deviation` = x$sdev, `Proportion of Variance` = vars, 
      `Cumulative Proportion` = cumsum(vars))
}

knitr::kable(pca_importance(sum.pca))

```

-   First 2 principal components explain about 50% of the cumulative variance in the dataset

-   First 4 principal components explain about 80% of the cumulative variance in the dataset

```{r pr_comp_wine, include = FALSE}
pr.wine$rotation <- -1*pr.wine$rotation
pr.wine$rotation


wine <- wine %>%
  mutate(color_quality = ifelse(color == "red" & quality >= 5, 1, ifelse(color == "red" & quality < 5, 2, ifelse(color == "white" & quality >= 5, 3, 4))))


```

```{r qplot_wine_pca, include = FALSE}
wine <- merge(wine, pr.wine$x[, 1:11], by = "row.names")
wine = subset(wine, select = -c(Row.names))

qplot(pr.wine$x[, 1], 
      pr.wine$x[, 2], 
      color = wine$color, 
      xlab = "PC 1", 
      ylab = "PC 2")
```

Clustering works better here for distinguishing reds with whites. The process is much more simple and pretty efficient at characterizing the wines. PCA is much harder to interpret because it is difficult to visualize more than two components at once with over 6500 observations. However, PCA suggests that the four clusters found in K-means are most explanatory of the data. This might confirm the "four-types" hypothesis I proposed from K-means.

# Problem 2: Market Segmentation

## Overview

## Organizing Followers by Tweets

```{r market_segmentation_data, include = FALSE}
social_marketing <- read.csv("../data/social_marketing.csv")
glimpse(social_marketing)
X <- subset(social_marketing, select = -X)

# Normalize phrase counts to phrase frequencies
X <- X/rowSums(X)
```

In terms of what we will do with the data, we have to find a suitable unsupervised method. Clustering makes the most sense here because we don't know much about the relationship between the categories. Understanding the components would be really tough. Similarly, in customer data a hierarchical model might not work as well for interpretability; there are too many observations. So, I proceed with K-means.

First, I normalize tweet counts as tweet *frequencies* to scale the observations for cluster analysis. To find the optimal number of clusters $k$ I produce a scree plot, or a plot showing the improvement in the within-sum-of-squares of the clusters as the number of clusters increases. I use the "elbow test," or I try to find the number of $k$ in the plot where there is a "crook" in the elbow.

```{r kmeans_market, include = FALSE}
# Focus on top 10 categories
# X <- subset(X[, names(top_10)], select = -chatter)

wss <- 0
set.seed(123)
for (i in 1:11) {
  km.social <- kmeans(X, centers = i, nstart = 20, iter.max = 50)
  wss[i] <- km.social$tot.withinss
}
```

```{r elbow_2, echo = FALSE}
plot(1:11, wss, type = 'b')
```

The scree plot indicates we should use two clusters.

### K-means with 2 clusters

Since there are so many observations and overlap within accounts, plotting the data was not very useful.

```{r social_km2, include = FALSE}

km.social <- kmeans(X, centers = 2, nstart = 20, iter.max = 50)


# Merge with clusters
X_clust <- rename(merge(X, km.social$cluster, by = "row.names"), cluster = y)


X_tbl <- X_clust %>%
  group_by(cluster) %>%
  select(-Row.names) %>%
  summarize_all(mean) %>%
  select(-c(cluster, chatter, uncategorized))
  
X_tbl <- as.data.frame(t(as.data.frame(X_tbl)))

X_tbl <- tibble::rownames_to_column(X_tbl)

colnames(X_tbl) <- c("Category", "cluster_1", "cluster_2") 

X_tbl$Category = as.factor(X_tbl$Category)

X_tbl <- X_tbl %>%
mutate(top_categoryc1 = ifelse(cluster_1 %in% head(sort(X_tbl$cluster_1, decreasing = TRUE), 5), TRUE, FALSE),
       top_categoryc2 = ifelse(cluster_2 %in% head(sort(X_tbl$cluster_2, decreasing = TRUE), 5), TRUE, FALSE))


p1 <- ggplot(X_tbl) + 
  geom_col(aes(x = Category, y = cluster_1, fill = top_categoryc1), show.legend = FALSE) + coord_flip() + 
  ylab("Average Frequency of Tweets") + 
  ggtitle("Cluster 1") + 
  scale_fill_manual(values = c("#999999", "#FF9999"))

p2 <- ggplot(X_tbl) + 
  geom_col(aes(x = Category, y = cluster_2, fill = top_categoryc2), show.legend = FALSE) + coord_flip() + 
  ggtitle("Cluster 2") + 
  ylab("Average Frequency of Tweets") + 
  scale_fill_manual(values = c("#999999", "#FF9999"))
  
figure <- ggarrange(p1, 
                    p2,
                    ncol = 2,
                    nrow = 1)  
figure  
```

It appears that we can categorize NutrientH20's followers into two categories by their tweets' categorical frequencies. In cluster 1, the top 5 tweet categories are photo sharing, personal fitness, outdoors, health/nutrition, and cooking. These seem like the types of people that the account can market to by promoting the health and fitness benefits of the drink. Maybe they can post a picture of someone hiking with their NutrientH20 drink!

In cluster 2, the top categories are travel, sports, politics, photo sharing, and current events. These are also probably the top categories for young men on Twitter in general. To market to this audience, NutrientH20 should just try to stay relevant in Twitter culture. People in cluster 2 would totally buy NutrientH20 just because one of their funny tweets went viral.

# Problem 3: Market Basket Analysis

```{r p3_data, include = FALSE}

groceries <- read_delim(
  file = "../data/groceries.txt",
  delim = "\t",
  col_names = FALSE
)
groceries <- gsub(",([A-Za-z])",
                  ", \\1",
                  groceries$X1)
groceries <- as.data.frame(groceries)
groceries$basket = as.factor(c(1:9835))
groceries <- split(x = groceries$groceries, f = groceries$basket)


groceries <- as(groceries, "list")

groceries[1]

for(i in 1:length(groceries)) {
  for(j in 1:length(groceries[i])) {
    groceries[i] = list(groceries[i])
    groceries[[i]][j] = groceries[[j]] 
  }
}
```

```{r data_ap, include = FALSE}

basket_trans <- as(groceries, "transactions")
summary(basket_trans)

# apriori

basketrules = apriori(basket_trans, 
                      parameter = list(support = 0.001, confidence = 0.001,
                           maxlen = 2))

```
