---
title: "Homework 3"
author: "Christina Ridlen"
date: "`r Sys.Date()`"
output: md_document
---

```{r libraries, include = FALSE}
library(tidymodels)
library(tidyverse)
library(glmnet)
library(DALEXtra)
library(ggmap)
```

# What causes what?

## Overview

Researchers from UPenn were interested in seeing if there was a causal relationship between police presence and crime *in that order*. They used "High Alert" notices in Washington D.C. as an instrumental variable for police presence, as police presence in this instance is related to a threat of terrorism rather than crime levels.

1.  The regression estimates will be highly biased because the correlation between crime and number of police in a city is extremely high. We could not establish causality with this regression.
2.  The researchers from UPenn found a scenario in which number of police is *not* caused by crime rate. Specifically, in Washington, D.C., the city is on "High Alert" if the risk of terrorism at the time is high. Thus the number of police stationed in the city is high and unrelated to the actual level of crime that occurs. Their regression shows that when policy presence is high, the crime level decreases.
3.  The researchers controlled for the possibility that people might not be outside in the D.C. area during High Alert times. In Table 2, they find that even after controlling for number of people active in the city, the level of crime still decreases at a significant level when police presence is high.
4.  The model being estimated here creates an interaction with high alert and district 1 (which I assume is the National Mall). Having a high presence of police in district 1 results in a much greater decrease in crime compared to having a high presence of police and crime in other districts. Once controlling for those interactions, the increase in crime from a high presence of foot traffic (measured by midday ridership) is less significant. So a decrease in crime is significantly dependent on location, not just the number of people around.

# Tree modeling: dengue cases

## Overview

The goal here is to compare three tree-based models for predicting dengue fever in Latin America. Specifically, we want to predict total cases in two Latin American cities: San Juan, Puerto Rico and Iquitos, Peru. We will be comparing the predictive value of a CART, Random Forests, and Boosted Tree model.

Our dataset contains variables related to the persistence of mosquito breeding and spreading in the area, which will be used as predictors. First, I created a train-test split of the data, and created folds for cross-validation from the training data. The testing data will be reserved for a final comparison of out-of-sample performance.

```{r dengue_data, include = FALSE}
# Load data
dengue <- read_csv("../data/dengue.csv")
# Remove missing values
dengue <- dengue[complete.cases(dengue),]
# Treat categorical variables
dengue$city <- factor(dengue$city)
dengue$season <- factor(dengue$season)
```

```{r dengue_fix, include = FALSE}
# Train-test split
set.seed(1)
dengue_split <- initial_split(dengue, prop = 0.8, strata = total_cases)
dengue_train <- training(dengue_split)
dengue_test <- testing(dengue_split)

# Folds for cross validation
folds <- vfold_cv(dengue_train, v = 10)
```

```{r CART, include = FALSE}

# Create regression tree
cart_spec <- decision_tree() %>%
  set_mode("regression") %>%
  set_engine("rpart")


# Choose best model
tune_spec <- decision_tree(tree_depth = tune(),
                           cost_complexity = tune()) %>%
  set_mode("regression") %>%
  set_engine("rpart")

# Tuning grid
tree_grid <- grid_regular(parameters(tune_spec),
                          levels = 4)
tree_grid

# Tune along the grid

tune_results <- tune_grid(tune_spec,
                          total_cases ~ .,
                          resamples = folds,
                          grid = tree_grid,
                          metrics = metric_set(rmse))

# Find best model
# Fit to training data
final_params <- select_best(tune_results)
best_spec <- finalize_model(tune_spec, final_params)
cart_final <- fit(best_spec,
                 total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt,
                 data = dengue_train)

cart_final 
view(final_params)


# In-sample performance
in_dengue_cart <- predict(cart_final, dengue_train) %>%
  bind_cols(dengue_train)

rmse_in_cart <- rmse(in_dengue_cart,
     estimate = .pred,
     truth = total_cases)

# Out-of-sample performance
out_dengue_cart <- predict(cart_final, dengue_test) %>%
  bind_cols(dengue_test)

rmse_out_cart <- rmse(out_dengue_cart,
     estimate = .pred,
     truth = total_cases)
rmse_out_cart
```

### Comparisons

The CART model was fit using a regression of total cases in the week on city, season and meteorological variables using the training data. I used the `tidymodels` package and `rpart` engine to specify the decision tree. After fitting the model, I identified the optimal cost complexity and tree depth parameters that would minimize the RMSE through cross-validation. The model was then pruned so that the final decision tree used the optimal parameters `cp = 1e=-04` and `tree_depth =5`. Finally, the in-sample performance was calculated after cross-validation using the training data, and out-of-sample performance was calculated using the testing data.

The same regression that was used in the CART model also is used to fit the random forest and boosted tree models. I calculated the in-sample and out-of-sample performances for this model. Fitting the boosted tree model was similar, and the in-sample and out-of-sample performances for the three models are summarized below.

```{r random_forests, include = FALSE}
# Specify model
rand_spec <- rand_forest() %>%
  set_mode("regression") %>%
  set_engine("ranger")

# Train model

model_rand <- rand_spec %>%
  fit(total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt,
      data = dengue_train)

# Cross-validated in sample performance

set.seed(50)
rand_cv <- fit_resamples(rand_spec,
                         total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt,
                         resamples = folds,
                         metrics = metric_set(rmse))
rmse_in_rand <- collect_metrics(rand_cv, summarize = TRUE)
rmse_in_rand

# Out-of-sample performance

out_dengue_rand <- predict(model_rand, dengue_test) %>%
  bind_cols(dengue_test)

rmse_out_rand <- rmse(out_dengue_rand,
                      estimate = .pred,
                      truth = total_cases) 
rmse_out_rand
```

```{r boosted, include = FALSE}

# Specify
boost_spec <- boost_tree() %>%
  set_mode("regression") %>%
  set_engine("xgboost")

# Train model
boost_model <- fit(boost_spec,
                   total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt,
                   dengue_train)

# CV
set.seed(100)
boost_cv <- fit_resamples(boost_spec,
                            total_cases ~ city + season + specific_humidity + tdtr_k + precipitation_amt,
                            resamples = folds,
                            metrics = metric_set(rmse))
rmse_in_boost <- collect_metrics(boost_cv, summarize = TRUE)
rmse_in_boost

# Out of Sample
out_dengue_boost <- predict(boost_model, dengue_test)  %>%
  bind_cols(dengue_test)

rmse_out_boost <- rmse(out_dengue_boost,
                       estimate = .pred,
                       truth = total_cases)
rmse_out_boost
```

```{r tree_table, include = FALSE}
# Create table listing in-sample and out of sample performance of tree models

rmse_outs <- c(c(rmse_out_cart), c(rmse_out_rand), c(rmse_out_boost))
rmse_ins <- c(c(rmse_in_cart), c(rmse_in_rand), c(rmse_in_boost))
```

| Model   | Out-Of-Sample RMSE | In-Sample RMSE |
|---------|--------------------|----------------|
| CART    | 25.3               | 24.4           |
| RF      | 24.3               | 26.21          |
| Boosted | 27.8               | 28.2           |

### Partial Dependence Plots

The random forests model has the lowest out of sample RMSE. So we use this model for the partial dependence plots:

```{r pdp, include = FALSE}

# one-hot encoding
data <- mltools::one_hot(data.table::data.table(dengue_train))
library(ranger)
model <- ranger(total_cases ~. -nvdi_sw -nvdi_ne - nvdi_nw - nvdi_se - air_temp_k - dewpoint_temp_k - max_air_temp_k - min_air_temp_k, data = data)
explainer = DALEX::explain(model, data = data, y = data$total_cases)
rf_precip <- model_profile(explainer = explainer, variables = c("precipitation_amt", "specific_humidity", "tdtr_k"), type = "partial")

rf_spec <- model_profile(explainer = explainer, variables = "specific_humidity", type = "partial")

rf_tdtr <- model_profile(explainer = explainer, variables = "tdtr_k", type = "partial")



```

```{r pdp_1, echo = FALSE}
## Plot precip with ggplot
as_tibble(rf_precip$agr_profiles) %>%
  ggplot(aes(`_x_`, `_yhat_`)) + 
  geom_line(size = 1.2, alpha = 0.8) +
  labs(title = "Partial dependence of total cases on precipitation amount",
       x = "Precipitation Amount",
       y = "Predicted number of cases")

```

```{r pdp_2, echo = FALSE}
## Plot spec with ggplot
as_tibble(rf_spec$agr_profiles) %>%
  ggplot(aes(`_x_`, `_yhat_`)) + 
  geom_line(size = 1.2, alpha = 0.8) + 
  labs(title = "Partial dependence of total cases on specific humidity",
       x = "Specific humidity",
       y = "Predicted number of cases")

```

```{r pdp_3, echo = FALSE}
## Plot tdtr
as_tibble(rf_tdtr$agr_profiles) %>%
  ggplot(aes(`_x_`, `_yhat_`)) + 
  geom_line(size = 1.2, alpha = 0.8) + 
  labs(title = "Partial dependence of total cases on average DTR",
       x = "Average Diurnal Temperature Range",
       y = "Predicted number of cases")
```

The final plot shows the partial dependence of total cases on average DTR, or Average Diurnal Temperature Range for the week. The other partial dependence plots show an obvious upward trend, while DTR shows a dip as temperature range increases. This is probably because temperature does not change much when it is humid. There is a slight increase at the upper end of the x axis, which may be because heavy rain can cool down temperatures. The results from the partial dependence plot provide more insight into which specific conditions will result in higher dengue cases.

# Green Certification

```{r green_data, include = FALSE}

# Load data
greenbuildings <- read_csv("../data/greenbuildings.csv")

# Categorical variables to factors
glimpse(greenbuildings)
greenbuildings <- greenbuildings %>%
  mutate(CS_PropertyID = factor(CS_PropertyID),
         cluster = factor(cluster))

# Create revenue per square foot variable
greenbuildings <- greenbuildings %>%
  mutate(revenue_sqft = Rent*leasing_rate)

# Remove rent and leasing rate
# greenbuildings <- subset(greenbuildings, select = -c(Rent, leasing_rate))

#Remove missing values
greenbuildings <- greenbuildings[complete.cases(greenbuildings),]


# Train test split
set.seed(100)
green_split <- initial_split(greenbuildings, prop = 0.8, strata = revenue_sqft)
green_train <- training(green_split)
green_test <- testing(green_split)
```

## Overview

In this problem, we are working with a dataset related to commerical rental properties across the United States. There are 7894 properties in total, and 685 of these have been awarded a LEED or EnergyStar certification is being a "green building." The dataset contains 23 predictor variables, containing information on the buildings themselves and details about the surrounding area.

The goal is to create the best predictive model for revenue per square foot per year, and to see how being green certified effects this outcome.

## Data and Results

First, I created the revenue per square foot variable `revenue_sqft` by multiplying `Rent` and `leasing_rate`. In order to isolate the effects of all other predictor variables, I naturally removed these variables from the dataset. However, upon running a Random Forest regression, the out-of-sample RMSE was more than twice the RMSE of the in-sample predictions. I then ran the Random Forest model again, this time including rent and leasing rate, to see what the issue was:

```{r green_rf, include = FALSE, cache = TRUE}
rand_spec <- rand_forest(
  mtry = tune(),
  trees = 500,
  min_n = tune()) %>%
  set_mode("regression") %>%
  set_engine("ranger", importance = "impurity")

# Get rid of the unknown
rf_param <- rand_spec %>%
  parameters()
rf_param <-
  rf_param %>%
  finalize(x = green_train)

# Create tuning grid
tunegrid_rand <- grid_regular(rf_param,
                              levels = 2)
green_folds <- vfold_cv(green_train, v = 5)
tune_results <- tune_grid(rand_spec,
                          revenue_sqft ~ .,
                          resamples = green_folds,
                          grid = tunegrid_rand,
                          metrics = metric_set(rmse))
best_rand_params <- select_best(tune_results)
final_spec <- finalize_model(rand_spec, best_rand_params)
rand_model <- final_spec %>% fit(formula = revenue_sqft ~ ., data = green_train)

# In-sample performance
pred_insample <- predict(rand_model,
                         green_train) %>%
  bind_cols(green_train)

rmse_in_rf <- rmse(pred_insample,
     estimate = .pred,
     truth = revenue_sqft)

# Out performance
pred_outsample <- predict(rand_model,
                          green_test) %>%
  bind_cols(green_test)

rmse_out_rf <- rmse(pred_outsample,
     estimate = .pred,
     truth = revenue_sqft)

# Variable importance plot
vip::vip(rand_model)
```

After fitting the random forests model, I created a variable importance plot, which confirmed that the rent and leasing rate variables were overwhelming any predictive power of the other variables. I perform a lasso regression next to allow R to select important variables itself.

```{r lasso_green, include = FALSE}

x = model.matrix(revenue_sqft ~ . -CS_PropertyID - cluster, data = greenbuildings)
y = greenbuildings$revenue_sqft

xtrain = model.matrix(revenue_sqft ~ . -CS_PropertyID - cluster, data = green_train)
ytrain = green_train$revenue_sqft

xtest = model.matrix(revenue_sqft ~ . -CS_PropertyID - cluster, data = green_test)
ytest = green_test$revenue_sqft

#### Lasso regression

grid <- 10^seq(10, -2, length  = 100)
lasso = glmnet(xtrain, ytrain, alpha = 1, lambda = grid)


# cross-validated lasso
set.seed(123)
cv_out <- cv.glmnet(xtrain, ytrain, alpha = 1)
best_l <-cv_out$lambda.min

# in-sample predictions
cv_pred <- predict(cv_out, c = best_l,
                   newx = xtrain)
rmse_cv <- mean(sqrt((cv_pred - ytrain)^2))
rmse_cv

# out of sample
lasso_pred <- predict(lasso, s = best_l,
                      newx = xtest)

rmse_lasso <- mean(sqrt((lasso_pred - ytest)^2))
rmse_lasso

# obtain coefficient estimates

lasso_out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso_coef <- predict(lasso, type = "coefficients", 
                      s = best_l)[1:20, ]
lasso_coef
lasso_coef["green_rating"]
```

```{r green_data_removed, include= FALSE}
# Load data
greenbuildings <- read_csv("../data/greenbuildings.csv")

# Categorical variables to factors
glimpse(greenbuildings)
greenbuildings <- greenbuildings %>%
  mutate(CS_PropertyID = factor(CS_PropertyID),
         cluster = factor(cluster))

# Create revenue per square foot variable
greenbuildings <- greenbuildings %>%
  mutate(revenue_sqft = Rent*leasing_rate)

# Remove rent and leasing rate
greenbuildings <- subset(greenbuildings, select = -c(Rent, leasing_rate))

#Remove missing values
greenbuildings <- greenbuildings[complete.cases(greenbuildings),]


# Train test split
set.seed(100)
green_split <- initial_split(greenbuildings, prop = 0.8, strata = revenue_sqft)
green_train <- training(green_split)
green_test <- testing(green_split)
```

```{r green_rf_wo, include = FALSE, cache = TRUE}

rand_spec <- rand_forest(
  mtry = tune(),
  trees = 500,
  min_n = tune()) %>%
  set_mode("regression") %>%
  set_engine("ranger", importance = "impurity")

# Get rid of the unknown
rf_param <- rand_spec %>%
  parameters()
rf_param <-
  rf_param %>%
  finalize(x = green_train)

# Create tuning grid
tunegrid_rand <- grid_regular(rf_param,
                              levels = 2)
green_folds <- vfold_cv(green_train, v = 5)
tune_results <- tune_grid(rand_spec,
                          revenue_sqft ~ .,
                          resamples = green_folds,
                          grid = tunegrid_rand,
                          metrics = metric_set(rmse))
best_rand_params <- select_best(tune_results)
final_spec <- finalize_model(rand_spec, best_rand_params)
rand_model <- final_spec %>% fit(formula = revenue_sqft ~ ., data = green_train)

# In-sample performance
pred_insample <- predict(rand_model,
                         green_train) %>%
  bind_cols(green_train)

rmse(pred_insample,
     estimate = .pred,
     truth = revenue_sqft)

# Out performance
pred_outsample <- predict(rand_model,
                          green_test) %>%
  bind_cols(green_test)

rmse(pred_outsample,
     estimate = .pred,
     truth = revenue_sqft)

```

```{r lasso_green_wo, include = FALSE}

x = model.matrix(revenue_sqft ~ . -CS_PropertyID - cluster, data = greenbuildings)
y = greenbuildings$revenue_sqft

xtrain = model.matrix(revenue_sqft ~ . -CS_PropertyID - cluster, data = green_train)
ytrain = green_train$revenue_sqft

xtest = model.matrix(revenue_sqft ~ . -CS_PropertyID - cluster, data = green_test)
ytest = green_test$revenue_sqft

#### Lasso regression

grid <- 10^seq(10, -2, length  = 100)
lasso = glmnet(xtrain, ytrain, alpha = 1, lambda = grid)


# cross-validated lasso
set.seed(123)
cv_out <- cv.glmnet(xtrain, ytrain, alpha = 1)
best_l <-cv_out$lambda.min

# in-sample predictions
cv_pred <- predict(cv_out, c = best_l,
                   newx = xtrain)
rmse_cv <- mean(sqrt((cv_pred - ytrain)^2))
rmse_cv

# out of sample
lasso_pred <- predict(lasso, s = best_l,
                      newx = xtest)

rmse_lasso <- mean(sqrt((lasso_pred - ytest)^2))
rmse_lasso

# obtain coefficient estimates

lasso_out <- glmnet(x, y, alpha = 1, lambda = grid)
lasso_coef <- predict(lasso, type = "coefficients", 
                      s = best_l)[1:20, ]
lasso_coef
lasso_coef["green_rating"]
```

The RMSE was high, but there was no longer evidence of overfitting. I then ran random forest and lasso regressions again, including rent and leasing rate in the regression.

+--------------------------------------------+------------+------------+
| Model                                      | RMSE in    | RMSE out   |
+============================================+============+============+
| Lasso (with rent, leasing rate)            | 125        | 128        |
+--------------------------------------------+------------+------------+
| Lasso (without rent, leasing rate)         | 622        | 625        |
+--------------------------------------------+------------+------------+
| Random Forest (with rent, leasing rate)    | 72         | 86         |
+--------------------------------------------+------------+------------+
| Random Forest (without rent, leasing rate) | 288        | 710        |
+--------------------------------------------+------------+------------+

What we notice in the table is that by excluding rent and leasing rate from the regression is that the variance of the estimates increases. This is obvious due to the overfitting of the random forest model even when R chooses the optimal tree depth and number of randomly selected predictors. The lasso regression performs similarly in and out of sample. But with the exclusion of rent and leasing rate, the model has a harder time predicting what the revenue per square foot of the building will be, so the variance increases. However, we can better isolate the effects of predictor variables that are not almost perfectly correlated with our response variable `revenue_sqft`. I include the lasso regression estimates both including and excluding rent and leasing rate:

+-----------------------------------------+-------------------------------------+------------------------------------------------+
| Lasso estimate of `green_rating` (with) | **Lasso estimate of `LEED` (with)** | **Lasso estimate of `green_rating` (without)** |
+=========================================+=====================================+================================================+
| 0                                       | -26                                 | 194                                            |
+-----------------------------------------+-------------------------------------+------------------------------------------------+

So holding all else constant, having a green rating increases revenue per square foot (per calendar year) by \$194.

I include the summary statistics of `revenue_sqft` for reference:

```{r summ_table, echo = FALSE}
summ <- round(c(summary(greenbuildings$revenue_sqft)))
summ_df <- data.frame(summ)
knitr::kable(summ_df, col.names = "Value")
```

# California Housing

Here, we are working with data on residential housing in the state of California. Each observation comes from a census tract, or a Census Bureau defined neighborhood. The dataset includes the latitude and longitude of each census tract, demographic statistics related to income and population, and information on the houses themselves. I build the best predictive model for determining house prices in California given the information from the census data.

## Working with the data

As always, I create a train-test split of the data and folds for cross-validation. However, two of our variables `totalRooms` and `totalBedrooms` represent total rooms and bedrooms *within* the census district, so I standardize these variables by the number of households in the district to turn them into averages.

```{r ca_data, include = FALSE}

caHousing <- read_csv("../data/CAhousing.csv")

# Average for total rooms by house
caHousing <- caHousing %>%
 mutate(avgRooms = round((totalRooms/households)),
        avgBedrooms = round(totalBedrooms/households))

# Drop totals
caHousing <- subset(caHousing, select = -c(totalRooms, totalBedrooms))

# Train-test split
ca_split <- initial_split(caHousing, prop = 0.8, strata = medianHouseValue)
ca_train <- training(ca_split)
ca_test <- testing(ca_split)

glimpse(caHousing)

# Create folds for cross-validation
folds <- vfold_cv(ca_train, v = 4)

```

```{r boost_spec, include = FALSE, cache = TRUE}

boost_spec <- boost_tree(
trees = 500,
learn_rate = tune(),
tree_depth = tune(),
sample_size = tune()) %>%
set_mode("regression") %>%
set_engine("xgboost")


tunegrid_boost <- grid_regular(parameters(boost_spec),
levels = 2) #to shorten time

tune_results <- tune_grid(boost_spec,
                          medianHouseValue ~ . - longitude - latitude,
                          resamples = folds, 
                          grid = tunegrid_boost,
                          metrics = metric_set(rmse))

# Select best parameters
best_boost_params <- select_best(tune_results)
final_boost_spec <- finalize_model(boost_spec, best_boost_params)
boost_model <- final_boost_spec %>% fit(formula = medianHouseValue ~ . - longitude - latitude, data = ca_train)

best_boost_params
boost_model

# CV in-sample performance
cv_boost <- fit_resamples(final_boost_spec,
                          medianHouseValue ~ . - longitude - latitude,
                          resamples = folds,
                          metrics = metric_set(rmse))

collect_metrics(cv_boost)

# Out-of-sample performance
boost_predictions <- boost_model %>%
  predict(ca_test) %>%
  bind_cols(ca_test)

rmse_out_boost <- rmse(boost_predictions,
                       estimate = .pred,
                       truth = medianHouseValue)
rmse_out_boost
```

```{r stepwise_ca, include = FALSE}

baseline_model <- lm(medianHouseValue ~ . - longitude - latitude, data = ca_train)

# stepwise selection
model_step <- stats::step(baseline_model, 
                   scope = ~(. - longitude - latitude)^2)

getCall(model_step)
coef(model_step)

modelr::rmse(model_step, ca_train)
modelr::rmse(model_step, ca_test)
```

For choosing the best predictive model, the two options that seemed most useful were gradient boosting or stepwise selection. The small number of predictors meant gradient boosting would run less of a risk of overfitting. However, stepwise selection seemed right for predicting house value because of the high possibility of interactions between variables that I can't see myself. To find the best predictive model, I compare the performance of a gradient boosted tree to a stepwise model.

The baseline model for stepwise selection was a linear regression of median house value on all variables excluding location coordinates.
I then created an aribitrary boost specification and found optimal hyperparameters using the `tune()` function and a tuning grid. This approach uses cross-validation to find the best possible boosted model. The optimal parameters selected were:

```{r boost_params, echo = FALSE}
best_boost_params <- data.frame(subset(best_boost_params, select = -c(.config)))
knitr::kable(best_boost_params, col.names = c("Tree Depth", "Learn Rate", "Sample Size"))
```

I also calculated the in-sample RMSE by cross validation, and the out-of-sample performance using the testing data. To summarize the results:

| Model              | In-Sample Performance | Out-of-sample Performance |
|--------------------|-----------------------|---------------------------|
| Boosted Tree       | 70591                 | 71379                     |
| Stepwise selection | 72923                 | 77602                     |

Clearly, the boosted tree performs better than the stepwise model.

## Mapping the results

Using the `ggmap` package, we can visualize the results on an actual map of California.

```{r resids, include = FALSE}
boost_predictions <- boost_predictions %>%
  mutate(resids = medianHouseValue - .pred)
```

```{r first_map, include = FALSE}

ca_map <- get_map(location = 'california', zoom = 6, maptype = "roadmap")

```

```{r data_map, echo = FALSE}
ggmap(ca_map) + 
  geom_point(aes(longitude, latitude, color = medianHouseValue), data = caHousing) + 
  scale_color_viridis_c(option = "viridis", labels = c("0", "100000", "200000", "300000", "400000", "500000"))
```

A map of the predictions on the testing data:

```{r predict_map, echo = FALSE}
ggmap(ca_map) + 
  geom_point(aes(longitude, latitude, color = .pred), data = boost_predictions) + 
  scale_color_viridis_c(option = "viridis", labels = c("0", "100000", "200000", "300000", "400000", "500000"))
```

A map of the residuals from the predictions:

```{r resids_map, echo = FALSE}
ggmap(ca_map) +
  geom_point(aes(longitude, latitude, color = resids), data = boost_predictions) + 
  scale_color_viridis_c(option = "viridis", labels = c("-500000," ,"-200000", "0", "200000", "500000"))
```

Looking at the residuals from the predictions on the test set, the model appears to have done a good job at predicting (except for some over-predictions).
